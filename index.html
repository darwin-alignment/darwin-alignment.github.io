<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Reward Steering with Evolutionary Heuristics for Decoding-time Alignment.">
  <meta name="keywords" content="alignment, llm, reward guided, ">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Reward Steering with Evolutionary Heuristics for Decoding-time Alignment</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Reward Steering with Evolutionary Heuristics for Decoding-time Alignment</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=RgUB8xgAAAAJ&hl=en">Chia-Yu Hung</a><sup>1</sup></span>
            <span class="author-block">
              <a href="https://scholar.google.com.sg/citations?user=jPfEvuQAAAAJ&hl=en">Navonil Majumder</a><sup>1</sup></span>
            <span class="author-block">
              <a href="https://scholar.google.com.sg/citations?user=4q8VxIIAAAAJ&hl=en">Ambuj Mehrish</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://soujanyaporia.github.io/">Soujanya Poria</a><sup>1</sup>,
            </span>
            
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Singapore University of Technology and Design</span>
          
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2406.15193"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <span class="link-block">
                <a href="https://github.com/declare-lab/darwin"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                </a>
              </span>
              <!-- Code Link. -->
              
    
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">DARWIN is a decode-time alignment technique that uses a reward-guided tree search framework to align the LLM and achieve comparable performance to preference optimization on 2 instruction following benchmarks.</span> 
      </h2>
      <img src="static/images/darwin_overview.png" alt="BUFFET teaser.">
    </div>
  </div>
</section>
<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Summary</h2>
        <div class="content has-text-justified">
          <p><b>The Problem: Alignment tax</b><br> Common preference optimization techniques aligns LLMs with human preferences through fine-tuning. However, this process can potentially degrade the model's performance on certain tasks. Additionally, as user preferences evolve over time, the LLM may become misaligned with current user expectations.
          <p><b>Aligning without finetuning </b><br> Decoding time alignment achieves alignment by treating LLM generation as a reward-guided  tree search problem. This approach trades inference latency for model alignment performance. Unlike previous decoding alignment methods that saw little benefit, this technique shows significant gains on instruction-following benchmarks.</p>
          <!-- <p> Can we train a model that can decide when to retrieve, judges if retrieved passages are indeed helpful and generates conditioned on them? Can we build a reliable instruction-following LM that can provide citations?</p> -->
          <p><b>What is <b><span style="color: red">Darwin</span></b>?</b><br>
           <span style="color: red"><b>Darwin</b></span> is a decode-time inference technique that uses a reward-guided tree search framework to align the LLM. It balances exploration and exploitation of rewards by combining different search strategies, which helps minimize reward over-optimization and improves model alignment. The technique employs a pre-existing reward model from RewardBench to guide the tree search process.
          
            <p><b>How good is <b><span style="color: red">Darwin</span></b>?</b><br>
            Experiments show that <b><span style="color: red">Darwin</span>  significantly outperforms other decode-time alignment techniques such as ARGS on two popular instruction-following benchmarks Alpaca Eval2 and MT-bench</b>. 
           Additionally, <span style="color: red">Darwin</span> outperforms direct alignment techniques such as DPO on MT-bench and achieve comparable performance on Alpaca Eval2.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Animation. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Reward Steering with Evolutionary Heuristics for Decoding-time Alignment</h2>
        <!-- Interpolating. -->
        <!-- <h3 class="title is-4">Overall Model Performance</h3> -->
        <div class="content has-text-justified">
          <p>
          <span style="color: red"><b>Darwin</b></span> is a new framework that frames decoding time alignment as  <span style="color: red"><b>reward guided tree search</b></span> and combines <b>tree exploration</b> and exploitation in the search process. In particular, tree exploration can be defined as 
          </p>
          <ul>
            <li><b>Sample N generation:</b> Exploring the tree N times guided by an instruction </li>
            <li><b>Instruction Mutation: </b>LLM are prompted to mutate instruction into N instruction through adding/removing details or rephrasing. Explore the tree N times guided by N instructions. </li>
          </ul>
          <b>Tree exploitation</b> can be defined as
          <ul>
            <li><b>Best-of-N:</b> Return the highest reward state among N state at any point in the tree.  </li>
            <li><b>Reward-guided beam replacement: </b>  Exploration that leads to low reward states are replaced by high reward states during the tree search. Search is continued from the high reward state.</li>
          </ul>
          Below, we show a detailed iteration of <span style="color: red"><b>Darwin</b></span> that combines tree exploration and exploitation technique. This process can be applied for multiple iteration, generating better outputs at each iteration. </br> 
          <img src="static/images/darwin_detail.png" alt="special tokens">
        </div>
        <div class="columns is-vcentered interpolation-panel">
        </div>
        <br/>
      
        <div class="content has-text-justified">
          <h3 class="title is-4">Inference </h3>
          <p>
           
          </p>
          <ul>
            <li><span style="color: red"><b>Instruction mutation: </b></span>Darwin mutates a given candidate instruction into several mutated instructions to explore the search tree more effectively via several instructions</li>
            <li><span style="color: red"><b>Tree-search with reward-guided beam replacement: </b></span> Darwin replace low reward states with higher reward states in the search process for every m tokens generated.</li>
            <li><span style="color: red"><b>Iterative evolution: </b></span> Darwin creates an archive containing past mutated instructions and samples the archive to perform mutation  and search again.</li>
          </ul>
        </div>
      
      </section>
      <section class="section">
        <div class="container is-max-desktop">
          <div class="columns is-centered">
            <div class="column is-full-width">
              <h2 class="title is-3">Results and Analysis</h2>
              <div class="columns is-vcentered interpolation-panel">
              </div>
              <br/>
              <div class="content has-text-justified">
                <h3 class="title is-4">Main Results</h3>
                <p>
                <span style="color: red"><b>Darwin</b></span> outperforms direct alignment methods on MT-bench and achieve comparable performance to direct alignment methods on Alpaca Eval2.
                </p>
                <img src="static/images/result.png" alt="results">
              </div>
               <div class="content has-text-justified">

                <h3 class="title is-4">Analysis</h3>
                <h4 class="title is-8">(A) Mutation Generally Improves Win Rate (WR)</h4>
                <p>Our results show that mutation is a robust exploration method, leading to discovering higher reward states, hence general improvement for WR.</p>

                <h4 class="title is-8">(B) Replacement Exploitation Benefits Length Controlled (LC) Win Rate</h4> 
                <p>Replacement Exploitation tends to generate shorter response compared to other methods, potentially due to exploring less states with exploitation. </p>
                <h4 class="title is-8">(C) Exploration and Exploitation</h4>
                <p>Darwin combines the advantage of exploration and exploitation, achieving a more consistent WR and LC, demostrating the balance between both methods. Furthermore, with more iterations, Darwin's performance on both LC and WR consistently increase unlike other method that saturates quickly.</p>
                <h4 class="title is-4">Impact of replacement period <i>m</i></h4>
                <img src="static/images/darwin_ablation.png" alt="results" style="display: block; margin: 0 auto; width: 50%; height: auto;">
                <p>
                  By adjusting the replacement period<i>m</i>, we can control the level of exploitation in the tree search process, balancing the trade-off between exploration and exploitation. In our main experiments, we did not tune <i>m</i>. However, our ablation study shows that increasing the replacement period <i>m</i> leads to increases in both the LC and WC of Darwin.


                </p>
            </section>
      

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@misc{hung2024reward,
      title={Reward Steering with Evolutionary Heuristics for Decoding-time Alignment}, 
      author={Chia-Yu Hung and Navonil Majumder and Ambuj Mehrish and Soujanya Poria},
      year={2024},
      eprint={2406.15193},
      archivePrefix={arXiv},
      primaryClass={id='cs.CL' full_name='Computation and Language' is_active=True alt_name='cmp-lg' in_archive='cs' is_general=False description='Covers natural language processing. Roughly includes material in ACM Subject Class I.2.7. Note that work on artificial languages (programming languages, logics, formal systems) that does not explicitly address natural-language issues broadly construed (natural-language processing, computational linguistics, speech, text retrieval, etc.) is not appropriate for this area.'}
}</code></pre>
  </div>
</section>

<section class="section" id="Acknowledgement">
  <div class="container is-max-desktop content">
    <h2 class="title">Acknowledgement</h2>
    <pre>This website is created base on <a>https://selfrag.github.io/</a> and <a>nerfies.github.io</a>
    </pre>
  </div>
</section>



<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/declare-lab/darwin" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
